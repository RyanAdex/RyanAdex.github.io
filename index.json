[{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536422400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1536422400,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"https://RyanAdex.github.io/tutorial/","publishdate":"2018-09-09T00:00:00+08:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":[],"content":" 1.前言 高精度电子地图也称为高分辨率地图(HD Map，High Definition Map)，是一种专门为无人驾驶服务的地图。与传统导航地图不同的是，高精度地图除了能提供的道路(Road)级别的导航信息外，还能够提供车道(Lane)级别的导航信息。无论是在信息的丰富度还是信息的精度方面，都是远远高于传统导航地图的。 目前市面上提供高精度地图的厂商有：tomtom、here、百度、高德等。 高精度地图流行的格式有很多种，有的厂商直接基于rndf地图增加属性来制作高精度地图，也有厂商使用osm格式增加属性来制作高精度地图。 对于ADAS系统，则有ADASIS定义了地图的数据模型及传输方式，以CAN作为传输通道。 OpenDRIVE是一种开放的文件格式, 用于路网的逻辑描述，常用于高精度地图的制作，百度Apollo则使用基于OpenDRIVE格式改进过的高精度地图。\n本文主要对OpenDRIVE文件格式进行简述，详情可参考：http://www.opendrive.org/docs/OpenDRIVEFormatSpecDelta_1.5M_vs_1.4H.pdf。\n2.正文 OpenDRIVE文件格式为XML，该XML文件种包含了很多地图信息，如Road、Junction、station等。\n主要结构如下：\n OpenDRIVE |-header | |-geoReference | |-offset |-road | |-link | | |-predecessor | | |-successor | | |-neighbor | |-type | | |-speed | |-planView | | |-geometry | | | |-line | | | |-spiral | | | |-arc | | | |-poly3 | | | |-paramPoly3 | |-elevationProfile | | |-elevation | |-lateralProfile | | |-superelevation | | |-crossfall | | |-shape | |-lanes | | |-laneOffset | | |-laneSection | | | |-left | | | | |-lane | | | | | |-link | | | | | | |-predecessor | | | | | | |-successor | | | | | |-width | | | | | |-border | | | | | |-roadMark | | | | | | | -sway | | | | | | | -type | | | | | | | | -line | | | | | | | -explicit | | | | | | | | -line | | | | | |-material | | | | | |-visibility | | | | | |-speed | | | | | |-access | | | | | |-height | | | | | |-rule | | | |-center | | | | |-lane | | | | | |-link | | | | | | |-predecessor | | | | | | |-successor | | | | | | |-predecessor | | | | | | |-successor | | | | | |-roadMark | | | | | | | -sway | | | | | | | -type | | | | | | | | -line | | | | | | | -explicit | | | | | | | | -line | | | |-right | | | | |-lane | | | | | |-link | | | | | | |-predecessor | | | | | | |-successor | | | | | |-width | | | | | |-border | | | | | |-roadMark | | | | | | | -sway | | | | | | | -type | | | | | | | | -line | | | | | | | -explicit | | | | | | | | -line | | | | | |-material | | | | | |-visibility | | | | | |-speed | | | | | |-access | | | | | |-height | | | | | |-rule | |-objects | | |-object | | | |-repeat | | | |-outlines | | | | |-outline | | | | | |-cornerRoad | | | | | |-cornerLocal | | | |-material | | | |-validity | | | |-parkingSpace | | | |-markings | | | | |-marking | | | | | |-cornerReference | | | |-borders | | | | |-border | | | | | |-cornerReference | | |-objectReference | | | |-validity | | |-tunnel | | | |-validity | | |-bridge | | | |-validity | |-signals | | |-signal | | | |-validity | | | |-dependency | | | |-reference | | | |-positionRoad | | | |-positionInertial | | |-signalReference | | | |-validity | |-surface | | |-CRG | |-railroad | | |-switch | | | |-mainTrack | | | |-sideTrack | | | |-partner |-controller | |-control |-junction | |-connection | | |-predecessor | | |-successor | | |-laneLink | |-priority | |-controller | |-surface | | |-CRG |-junctionGroup | |-junctionReference |-station | |-platform | | |-segment  A.坐标系 最先考虑的应该时坐标系的表达方式，在GIS中一般使用两种常用的坐标系类型：\n 全局坐标系或球坐标系，例如经纬度。这些坐标系通常称为地理坐标系。(GCS)   基于横轴墨卡托、亚尔勃斯等积或罗宾森等地图投影的投影坐标系，这些地图投影（以及其他多种地图投影模型）提供了各种机制将地球球面的地图投影到二维笛卡尔坐标平面上。(PCS)  在OpenDRIVE中可表示为：\n\u0026lt;geoReference\u0026gt; \u0026lt;![CDATA[+proj=utm +zone=32 +ellps=WGS84 +datum=WGS84 +units= m+no_defs]]\u0026gt; \u0026lt;/geoReference\u0026gt;  从OpenDRIVE 1.4 开始, 可以使用格式化为 \u0026ldquo;proj4\u0026rdquo;-字符串的投影定义对路网进行地理参照转化. PROJ 是一种通用坐标变换软件, 它将地理空间坐标从一个坐标参考系统 (CRS) 转换为另一个坐标参考系统。这包括制图投影和大地测量转换。 geoReference元素定义了该文件使用的投影坐标系，其中地理坐标系为WGS-84。\n在OpenDRIVE数据中大量使用的位置信息都是投影后的xy坐标，而除了该投影坐标系，还定义了一种轨迹坐标系，如下所示，s坐标是沿着reference line的，关于reference line后面介绍，长度是在xy坐标下计算的。 t坐标，是相对于reference line的侧向位置，左正，右负。 B.Road Layout OpenDRIVE中路网结构中的一个road，该road有三部分组成，蓝色的reference line，车道lane，车道lane的其他feature(限速等)。\n所有道路都由一条参照线组成, 用于定义基本几何 (弧线、直线等)。沿着参考线, 可以定义道路的各种属性。这些是, 例如海拔概况、车道、交通标志等。道路可以直接连接 (当两个给定的道路之间只有一个连接时), 也可以通过路口 (当从某一道路到其他道路有一个以上的连接时)。\n所有属性都可以根据本规范中规定的标准进行参数化, 也可以通过用户定义的数据进行参数化。\nC.Reference Line 整个地图路网由很多的road构成，而每个road中都会包含reference line，就是一条线，它没有宽度。 reference line，线条有好几种类型，直线，螺旋线等， The geometry of the reference line is described as a sequence of primitives of various types. The available primitives are:\nstraight line (constant zero curvature) spiral (linear change of curvature) curve (constant non-zero curvature along run-length) cubic polynom parametric cubic curves\n下图为几种常见的reference line，注意图中的两个坐标系,xy和st\nD.Lane 车道是由数字识别的, 这些数字是唯一的 (每个车道部分, 见下文) - 顺序 (即没有缝隙), - 从参考线上的0开始 - 向左上升 (正 t 方向) - 向右下降 (负 t 方向)\n车道总数不受限制。参考线本身被定义为车道零, 不能有宽度条目 (即其宽度必须始终为 0.0)。\nE.Road Linkage road之间的连接定义了两种(每个road有唯一的ID)，一种是有明确的连接关系，例如前后只有一条road，那么通过 successor/predecessor进行连接(例如下图中的road 1和road 2)。\n3.总结 总之，对于一个road来说，先确定reference line，有了reference line的几何形状和位置，然后再确定reference line左右的车道lane,车道lane又有实线和虚线等属性；road 和road之间通过普通连接和Junction进行连接，同时还要将road中的相关车道进行连接。\n","date":1559657138,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1559657138,"objectID":"918013f7868aee0cb2e96ce32c60b414","permalink":"https://RyanAdex.github.io/2019/06/04/opendrive/","publishdate":"2019-06-04T22:05:38+08:00","relpermalink":"/2019/06/04/opendrive/","section":"post","summary":"1.前言 高精度电子地图也称为高分辨率地图(HD Map，High Definition Map)，是一种专门为无人驾驶服务的地图。与传统导航地图不同的是，高精度地图","tags":["XML"],"title":"Opendrive地图数据解析","type":"post"},{"authors":null,"categories":[],"content":"  参考：https://www.pyimagesearch.com/2018/07/30/opencv-object-tracking/\n 前言 Opencv作为图像处理开源库包含了Object Tracking目标追踪的一些API，使用Opencv能够方便快捷的编写目标追踪程序。\nOpencv4.0目前包含了8种目标追踪算法：\n Boosting：基于在线的AdaBoost, 这个分类器需要对对象的正、负例进行训练。用户提供的初始化框(或通过其它算法检测到对象，必须MOG2，KNN检测到小车)来作为对象的正例，并将边界框外的图像块作为背景(负类) 优点：没有。这个算法已经有10年的历史了，找不到一个很好的理由去使用它，特别是当其他基于类似原理的高级跟踪器(MIL, KCF)也可用的时候。 CSRT：判别性相关滤波器。 优点：精确度比KCF稍高。 缺点：速度不如KCF块。 GOTURN：在跟踪器类的所有跟踪算法中，这是唯一基于卷积神经网络(CNN)的算法。也是唯一一个使用离线训练的模型，因此它比其他跟踪器更快。从opencv文档可以看出该算法对视角变化、光照、变形都具有很好的鲁棒性，但是对于遮挡性能较差。 KCF：这个跟踪器基于前面两个跟踪器中提出的想法。该跟踪器在MIL跟踪器中使用的多个正样本具有较大的重叠区域。 优点：精度和速度都比MIL好，建议在大多数应用程序中使用该算法。 缺点：还是完全遮挡 MedianFlow：经过测试中，发现这个跟踪器在小运动情况下表现最好。不像其他跟踪器，即使跟踪失败了还继续跟踪，这个跟踪器知道什么时候失败。 优点：跟踪失败报告，小运动下表现好 缺点：大运动，该算法失灵 MIL：这个跟踪器与上面描述的boost跟踪器类似。最大的区别是，它不是只考虑对象的当前位置作为正类，还考虑当前位置邻域范围的潜在位置作为正类。 优点： 性能很好。它不boosting跟踪器那样，在部分遮挡下依然表现挺佳。但效果不如KCF好。 缺点：跟踪失败没有可靠的报告。完全遮挡的话性能差 MOSSE：如果对速度要求非常高，MOSSE可能是你的更好选择。 优点：速度比CSRT和KCF都要快。 缺点：精度没有CSRT和KCF高。 TLD：TLD stands for Tracking, learning and detection. 如果有一个视频序列，对象隐藏在另一个对象后面，这个跟踪器可能是一个不错的选择。 优点：在多帧的情况下，在遮挡的情况下工作最好。此外，该算法能很好的应对尺度变化。\n缺点：大量的假阳性使得其几乎无法使用。   我的个人建议是：\n 当您需要更高的目标跟踪精度并可承受较慢的fps吞吐量时，请使用CSRT 当您需要更快的fps吞吐量，但可以处理稍低的对象跟踪精度时，请使用KCF 当您需要纯速度时使用MOSSE  使用Opencv进行对象跟踪 确保电脑已经安装了opencv4.0。 要使用Opencv执行对象跟踪，请引入以下头文件：\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;opencv.hpp\u0026gt; #include \u0026lt;tracking.hpp\u0026gt;  首先要创建一个跟踪器并实例化，本文仅使用KCF跟踪器作为示例：\ncv::Ptr\u0026lt;cv::Tracker\u0026gt; tracker;//跟踪器 tracker = cv::TrackerKCF::create();  接下来，我们初始化视频流，并定义显示窗口：\ncv::Mat frame;//保存每帧图像 cv::VideoCapture cap(\u0026quot;run.mp4\u0026quot;); //创建窗口 static const string kWinName = \u0026quot;object tracking in OpenCV\u0026quot;; cv::namedWindow(kWinName,cv::WINDOW_AUTOSIZE); if(!cap.isOpened()){ cout \u0026lt;\u0026lt;\u0026quot;Could not read video file\u0026quot;\u0026lt;\u0026lt;endl; return -1; }  为了能够在视频中选取跟踪目标，需要定义一个初始化ROI区域：\ncv::Rect2d initBB;//初始目标坐标  从视频流循环帧分析：\nwhile(key=cv::waitKey(1)){ //读取frame cap\u0026gt;\u0026gt;frame; //如果播放完则停止 if(frame.empty()){ break; } if(key=='s'){ if(!tracker-\u0026gt;empty()){ tracker.release(); tracker = cv::TrackerKCF::create(); } //选择初始目标区域 initBB=cv::selectROI(kWinName,frame,true,false); tracker-\u0026gt;init(frame,initBB); } if(tracker-\u0026gt;update(frame,initBB)){ cv::rectangle(frame,initBB,cv::Scalar(255,0,0),3); } cv::imshow(kWinName,frame); }  为了更快的处理数据，我们可以将读取的frame进行cv::resize（）操作调整图片大小，该用法文章不做说明。因为处理的数据越少，我们的对象跟踪管道的运行速度就越快。 在跟踪前我们需要使用快捷键s来对跟踪目标进行选取并初始化，考虑到在视频流中可能进行多次跟踪目标初始化，所以需要加入：\nif(!tracker-\u0026gt;empty()){ tracker.release(); tracker = cv::TrackerKCF::create(); }  以确保跟踪器是新的。 如果选择了某个对象，我们需要跟新该对象的位置，因为每一帧对于跟踪器都会重新计算跟踪目标的位置。使用update（）方法将定位对象的新位置，并返回true和边界box对象。\n代码如下：\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;opencv.hpp\u0026gt; #include \u0026lt;tracking.hpp\u0026gt; using namespace std; int main(int argc, char const *argv[]) { cv::Ptr\u0026lt;cv::Tracker\u0026gt; tracker;//跟踪器 cv::Mat frame;//保存每帧图像 char key;//快捷键 tracker = cv::TrackerKCF::create(); cv::Rect2d initBB;//初始目标坐标 //Read video cv::VideoCapture cap(\u0026quot;D:\\\\Code\\\\C++\\\\YOLO3-detecction\\\\run.mp4\u0026quot;); // cv::VideoCapture cap(0); //创建窗口 static const string kWinName = \u0026quot;object tracking in OpenCV\u0026quot;; cv::namedWindow(kWinName,cv::WINDOW_AUTOSIZE); if(!cap.isOpened()){ cout \u0026lt;\u0026lt;\u0026quot;Could not read video file\u0026quot;\u0026lt;\u0026lt;endl; return -1; } while(key=cv::waitKey(1)){ //读取frame cap\u0026gt;\u0026gt;frame; cv::resize(frame,frame,cv::Size(frame.cols/2,frame.rows/2),0,0,cv::INTER_LINEAR); //如果播放完则停止 if(frame.empty()){ break; } if(key=='s'){ if(!tracker-\u0026gt;empty()){ tracker.release(); tracker = cv::TrackerKCF::create(); } //选择初始目标区域 initBB=cv::selectROI(kWinName,frame,true,false); tracker-\u0026gt;init(frame,initBB); } if(tracker-\u0026gt;update(frame,initBB)){ cv::rectangle(frame,initBB,cv::Scalar(255,0,0),3); } cv::imshow(kWinName,frame); } tracker.release(); return 0; }  ","date":1547969651,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1547969651,"objectID":"d8ed2c65071af3a51dee56d47119d802","permalink":"https://RyanAdex.github.io/2019/01/20/opencv-tracker/","publishdate":"2019-01-20T15:34:11+08:00","relpermalink":"/2019/01/20/opencv-tracker/","section":"post","summary":"参考：https://www.pyimagesearch.com/2018/07/30/opencv-object-tracking/ 前言 O","tags":["C++","opencv"],"title":"Opencv目标追踪","type":"post"},{"authors":null,"categories":[],"content":" 1. 前言 YOLO3能够快速识别图片和视频中的80种物体，而且实时性强，准确度接近SSD。 Opencv是目前最流行的开源图像处理库，使用Opencv能够非常方便的对图像进行处理。 Opencv4.0已经包含DNN相关的库函数，可以非常方便的调用训练好的YOLO3模型使用。\n2. 采用YOLO3目标检测 第一步：下载模型 使用wget下载训练号的模型与COCO数据库。 COCO数据库包含了识别的类型名。\nwget https://pjreddie.com/media/files/yolov3.weights wget https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg?raw=true -O ./yolov3.cfg wget https://github.com/pjreddie/darknet/blob/master/data/coco.names?raw=true -O ./coco.names  第二步：初始化参数 YOLOv3算法的预测结果就是边界框。每一个边界框都旁随着一个置信值。第一阶段中，全部低于置信度阀值的都会排除掉。 对剩余的边界框执行非最大抑制算法，以去除重叠的边界框。非最大抑制由一个参数nmsThrehold控制。读者可以尝试改变这个数值，观察输出的边界框的改变。 接下来，设置输入图片的宽度（inpWidth）和高度（inpHeight）。我们设置他们为416，以便对比YOLOv3作者提供的Darknets的C代码。如果想要更快的速度，读者可以把宽度和高度设置为320。如果想要更准确的结果，改变他们到608。\nfloat confThreshold = 0.5;//置信度阈值 float nmsThreshold = 0.4;//非最大抑制阈值 int inpWidth = 416;//网络输入图片宽度 int inpHeight = 416;//网络输入图片高度  第三步：读取模型和COCO数据库 接下来我们读入COCO数据库并将类名存入vector classes容器。并加载模型与权重文件yolov3.cfg和yolov3.weights。 最后把DNN的后端设置为OpenCV，目标设置CPU。也可以通过cv::dnn::DNN_TARGET_OPENCL设置目标为GPU。\n//将类名存进容器 string classesFile = \u0026quot;D:\\\\Code\\\\C++\\\\YOLO3-detecction\\\\coco.names\u0026quot;;//coco.names包含80种不同的类名 ifstream ifs(classesFile.c_str()); string line; while(getline(ifs,line))classes.push_back(line); //取得模型的配置和权重文件 cv::String modelConfiguration = \u0026quot;D:\\\\Code\\\\C++\\\\YOLO3-detecction\\\\yolov3.cfg\u0026quot;; cv::String modelWeights = \u0026quot;D:\\\\Code\\\\C++\\\\YOLO3-detecction\\\\yolov3.weights\u0026quot;; //加载网络 cv::dnn::Net net = cv::dnn::readNetFromDarknet(modelConfiguration,modelWeights); net.setPreferableBackend(cv::dnn::DNN_BACKEND_OPENCV); net.setPreferableBackend(cv::dnn::DNN_TARGET_OPENCL);  第四步：读取输入 //打开视频文件或者图形文件或者相机数据流 string str, outputFile; cv::VideoCapture cap(\u0026quot;D:\\\\Code\\\\C++\\\\YOLO3-detecction\\\\run.mp4\u0026quot;); cv::VideoWriter video; cv::Mat frame,blob; //开启摄像头 // cv::VideoCapture cap(1); //创建窗口 static const string kWinName = \u0026quot;Deep learning object detection in OpenCV\u0026quot;; cv::namedWindow(kWinName,cv::WINDOW_AUTOSIZE);  第五步：处理每一帧 输入到神经网络的图像需要以一种叫bolb的格式保存。 读取了输入图片或者视频流的一帧图像后，这帧图像需要经过bolbFromImage()函数处理为神经网络的输入类型bolb。在这个过程中，图像像素以一个1/255的比例因子，被缩放到0到1之间。同时，图像在不裁剪的情况下，大小调整到416x416。注意我们没有降低图像平均值，因此传递[0,0,0]到函数的平均值输入，保持swapRB参数到默认值1。 输出的bolb传递到网络，经过网络正向处理，网络输出了所预测到的一个边界框清单。这些边界框通过后处理，滤除了低置信值的。我们随后再详细的说明后处理的步骤。我们在每一帧的左上方打印出了推断时间。伴随着最后的边界框的完成，图像保存到硬盘中，之后可以作为图像输入或者通过Videowriter作为视频流输入。\nwhile(cv::waitKey(1)\u0026lt;0){ //取每帧图像 cap\u0026gt;\u0026gt;frame; //如果视频播放完则停止程序 if(frame.empty()){ break; } //在dnn中从磁盘加载图片 cv::dnn::blobFromImage(frame,blob,1/255.0,cv::Size(inpWidth,inpHeight)); //设置输入 net.setInput(blob); //设置输出层 vector\u0026lt;cv::Mat\u0026gt; outs;//储存识别结果 net.forward(outs,getOutputNames(net)); //移除低置信度边界框 postprocess(frame,outs); //显示s延时信息并绘制 vector\u0026lt;double\u0026gt; layersTimes; double freq = cv::getTickFrequency()/1000; double t=net.getPerfProfile(layersTimes)/freq; string label = cv::format(\u0026quot;Infercence time for a frame:%.2f ms\u0026quot;,t); cv::putText(frame,label,cv::Point(0,15),cv::FONT_HERSHEY_SIMPLEX,0.5,cv::Scalar(0,255,255)); //绘制识别框 cv::imshow(kWinName,frame); }  第五a步：得到输出层名字 OpenCV的网络类中的前向功能需要结束层，直到它在网络中运行。因为我们需要运行整个网络，所以我们需要识别网络中的最后一层。我们通过使用getUnconnectedOutLayers()获得未连接的输出层的名字，该层基本就是网络的最后层。然后我们运行前向网络，得到输出，如前面的代码片段（net.forward(getOutputsNames(net))）。\n//从输出层得到名字 vector\u0026lt;cv::String\u0026gt; getOutputNames(const cv::dnn::Net\u0026amp; net){ static vector\u0026lt;cv::String\u0026gt; names; if(names.empty()){ //取得输出层指标 vector\u0026lt;int\u0026gt; outLayers = net.getUnconnectedOutLayers(); vector\u0026lt;cv::String\u0026gt; layersNames = net.getLayerNames(); //取得输出层名字 names.resize(outLayers.size()); for(size_t i =0;i\u0026lt;outLayers.size();i++){ names[i] = layersNames[outLayers[i]-1]; } } return names; }  第五b步：后处理网络输出 网络输出的每个边界框都分别由一个包含着类别名字和5个元素的向量表示。 头四个元素代表center_x, center_y, width和height。第五个元素表示包含着目标的边界框的置信度。 其余的元素是和每个类别（如目标种类）有关的置信度。边界框分配给最高分数对应的那一种类。 一个边界框的最高分数也叫做它的置信度（confidence）。如果边界框的置信度低于规定的阀值，算法上不再处理这个边界框。 置信度大于或等于置信度阀值的边界框，将进行非最大抑制。这会减少重叠的边界框数目。\n//移除低置信度边界框 void postprocess(cv::Mat\u0026amp; frame,const vector\u0026lt;cv::Mat\u0026gt;\u0026amp; outs){ vector\u0026lt;int\u0026gt; classIds;//储存识别类的索引 vector\u0026lt;float\u0026gt; confidences;//储存置信度 vector\u0026lt;cv::Rect\u0026gt; boxes;//储存边框 for(size_t i=0;i\u0026lt;outs.size();i++){ //从网络输出中扫描所有边界框 //保留高置信度选框 //目标数据data:x,y,w,h为百分比，x,y为目标中心点坐标 float* data = (float*)outs[i].data; for(int j=0;j\u0026lt;outs[i].rows;j++,data+=outs[i].cols){ cv::Mat scores = outs[i].row(j).colRange(5,outs[i].cols); cv::Point classIdPoint; double confidence;//置信度 //取得最大分数值与索引 cv::minMaxLoc(scores,0,\u0026amp;confidence,0,\u0026amp;classIdPoint); if(confidence\u0026gt;confThreshold){ int centerX = (int)(data[0]*frame.cols); int centerY = (int)(data[1]*frame.rows); int width = (int)(data[2]*frame.cols); int height = (int)(data[3]*frame.rows); int left = centerX-width/2; int top = centerY-height/2; classIds.push_back(classIdPoint.x); confidences.push_back((float)confidence); boxes.push_back(cv::Rect(left, top, width, height)); } } } //低置信度 vector\u0026lt;int\u0026gt; indices;//保存没有重叠边框的索引 //该函数用于抑制重叠边框 cv::dnn::NMSBoxes(boxes,confidences,confThreshold,nmsThreshold,indices); for(size_t i=0;i\u0026lt;indices.size();i++){ int idx = indices[i]; cv::Rect box = boxes[idx]; drawPred(classIds[idx],confidences[idx],box.x,box.y, box.x+box.width,box.y+box.height,frame); } }  非最大抑制由参数nmsThreshold控制。如果nmsThreshold设置太少，比如0.1，我们可能检测不到相同或不同种类的重叠目标。如果设置得太高，比如1，可能出现一个目标有多个边界框包围。所以我们在上面的代码使用了0.4这个中间的值。\n第五c步：画出计算得到的边界框 最后，经过非最大抑制后，得到了边界框。我们把边界框在输入帧上画出，并标出种类名和置信值。\n//绘制预测边界框 void drawPred(int classId,float conf,int left,int top,int right,int bottom,cv::Mat\u0026amp; frame){ //绘制边界框 cv::rectangle(frame,cv::Point(left,top),cv::Point(right,bottom),cv::Scalar(255,178,50),3); string label = cv::format(\u0026quot;%.2f\u0026quot;,conf); if(!classes.empty()){ CV_Assert(classId \u0026lt; (int)classes.size()); label = classes[classId]+\u0026quot;:\u0026quot;+label;//边框上的类别标签与置信度 } //绘制边界框上的标签 int baseLine; cv::Size labelSize = cv::getTextSize(label,cv::FONT_HERSHEY_SIMPLEX,0.5,1,\u0026amp;baseLine); top = max(top,labelSize.height); cv::rectangle(frame,cv::Point(left,top-round(1.5*labelSize.height)),cv::Point(left+round(1.5*labelSize.width),top+baseLine),cv::Scalar(255,255,255),cv::FILLED); cv::putText(frame, label,cv::Point(left, top), cv::FONT_HERSHEY_SIMPLEX, 0.75,cv::Scalar(0, 0, 0), 1); }  3. 全部代码 #include \u0026lt;iostream\u0026gt; #include \u0026lt;fstream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;opencv.hpp\u0026gt; using namespace std; vector\u0026lt;string\u0026gt; classes;//储存名字的容器 float confThreshold = 0.5;//置信度阈值 float nmsThreshold = 0.4;//非最大抑制阈值 int inpWidth = 416;//网络输入图片宽度 int inpHeight = 416;//网络输入图片高度 //移除低置信度边界框 void postprocess(cv::Mat\u0026amp; frame,const vector\u0026lt;cv::Mat\u0026gt;\u0026amp; out); //画出预测边界框 void drawPred(int classId,float conf,int left,int top,int right,int bottom,cv::Mat\u0026amp; frame); //取得输出层的名字 vector\u0026lt;cv::String\u0026gt; getOutputNames(const cv::dnn::Net\u0026amp; net); main(int argc, char const *argv[]) { //将类名存进容器 string classesFile = \u0026quot;D:\\\\Code\\\\C++\\\\YOLO3-detecction\\\\coco.names\u0026quot;;//coco.names包含80种不同的类名 ifstream ifs(classesFile.c_str()); string line; while(getline(ifs,line))classes.push_back(line); //取得模型的配置和权重文件 cv::String modelConfiguration = \u0026quot;D:\\\\Code\\\\C++\\\\YOLO3-detecction\\\\yolov3.cfg\u0026quot;; cv::String modelWeights = \u0026quot;D:\\\\Code\\\\C++\\\\YOLO3-detecction\\\\yolov3.weights\u0026quot;; //加载网络 cv::dnn::Net net = cv::dnn::readNetFromDarknet(modelConfiguration,modelWeights); net.setPreferableBackend(cv::dnn::DNN_BACKEND_OPENCV); net.setPreferableBackend(cv::dnn::DNN_TARGET_CPU); //打开视频文件或者图形文件或者相机数据流 string str, outputFile; cv::VideoCapture cap(\u0026quot;D:\\\\Code\\\\C++\\\\YOLO3-detecction\\\\run.mp4\u0026quot;); cv::VideoWriter video; cv::Mat frame,blob; //开启摄像头 // cv::VideoCapture cap(1); //创建窗口 static const string kWinName = \u0026quot;Deep learning object detection in OpenCV\u0026quot;; cv::namedWindow(kWinName,cv::WINDOW_AUTOSIZE); //处理每帧 while(cv::waitKey(1)\u0026lt;0){ //取每帧图像 cap\u0026gt;\u0026gt;frame; //如果视频播放完则停止程序 if(frame.empty()){ break; } //在dnn中从磁盘加载图片 cv::dnn::blobFromImage(frame,blob,1/255.0,cv::Size(inpWidth,inpHeight)); //设置输入网络 net.setInput(blob); //设置输出层 vector\u0026lt;cv::Mat\u0026gt; outs;//储存识别结果 net.forward(outs,getOutputNames(net)); //移除低置信度边界框 postprocess(frame,outs); //显示s延时信息并绘制 vector\u0026lt;double\u0026gt; layersTimes; double freq = cv::getTickFrequency()/1000; double t=net.getPerfProfile(layersTimes)/freq; string label = cv::format(\u0026quot;Infercence time for a frame:%.2f ms\u0026quot;,t); cv::putText(frame,label,cv::Point(0,15),cv::FONT_HERSHEY_SIMPLEX,0.5,cv::Scalar(0,255,255)); //绘制识别框 cv::Mat detecteFrame; frame.convertTo(detecteFrame,CV_8U); cv::imshow(kWinName,frame); } cap.release(); return 0; } //移除低置信度边界框 void postprocess(cv::Mat\u0026amp; frame,const vector\u0026lt;cv::Mat\u0026gt;\u0026amp; outs){ vector\u0026lt;int\u0026gt; classIds;//储存识别类的索引 vector\u0026lt;float\u0026gt; confidences;//储存置信度 vector\u0026lt;cv::Rect\u0026gt; boxes;//储存边框 for(size_t i=0;i\u0026lt;outs.size();i++){ //从网络输出中扫描所有边界框 //保留高置信度选框 //目标数据data:x,y,w,h为百分比，x,y为目标中心点坐标 float* data = (float*)outs[i].data; for(int j=0;j\u0026lt;outs[i].rows;j++,data+=outs[i].cols){ cv::Mat scores = outs[i].row(j).colRange(5,outs[i].cols); cv::Point classIdPoint; double confidence;//置信度 //取得最大分数值与索引 cv::minMaxLoc(scores,0,\u0026amp;confidence,0,\u0026amp;classIdPoint); if(confidence\u0026gt;confThreshold){ int centerX = (int)(data[0]*frame.cols); int centerY = (int)(data[1]*frame.rows); int width = (int)(data[2]*frame.cols); int height = (int)(data[3]*frame.rows); int left = centerX-width/2; int top = centerY-height/2; classIds.push_back(classIdPoint.x); confidences.push_back((float)confidence); boxes.push_back(cv::Rect(left, top, width, height)); } } } //低置信度 vector\u0026lt;int\u0026gt; indices;//保存没有重叠边框的索引 //该函数用于抑制重叠边框 cv::dnn::NMSBoxes(boxes,confidences,confThreshold,nmsThreshold,indices); for(size_t i=0;i\u0026lt;indices.size();i++){ int idx = indices[i]; cv::Rect box = boxes[idx]; drawPred(classIds[idx],confidences[idx],box.x,box.y, box.x+box.width,box.y+box.height,frame); } } //绘制预测边界框 void drawPred(int classId,float conf,int left,int top,int right,int bottom,cv::Mat\u0026amp; frame){ //绘制边界框 cv::rectangle(frame,cv::Point(left,top),cv::Point(right,bottom),cv::Scalar(255,178,50),3); string label = cv::format(\u0026quot;%.2f\u0026quot;,conf); if(!classes.empty()){ CV_Assert(classId \u0026lt; (int)classes.size()); label = classes[classId]+\u0026quot;:\u0026quot;+label;//边框上的类别标签与置信度 } //绘制边界框上的标签 int baseLine; cv::Size labelSize = cv::getTextSize(label,cv::FONT_HERSHEY_SIMPLEX,0.5,1,\u0026amp;baseLine); top = max(top,labelSize.height); cv::rectangle(frame,cv::Point(left,top-round(1.5*labelSize.height)),cv::Point(left+round(1.5*labelSize.width),top+baseLine),cv::Scalar(255,255,255),cv::FILLED); cv::putText(frame, label,cv::Point(left, top), cv::FONT_HERSHEY_SIMPLEX, 0.75,cv::Scalar(0, 0, 0), 1); } //从输出层得到名字 vector\u0026lt;cv::String\u0026gt; getOutputNames(const cv::dnn::Net\u0026amp; net){ static vector\u0026lt;cv::String\u0026gt; names; if(names.empty()){ //取得输出层指标 vector\u0026lt;int\u0026gt; outLayers = net.getUnconnectedOutLayers(); vector\u0026lt;cv::String\u0026gt; layersNames = net.getLayerNames(); //取得输出层名字 names.resize(outLayers.size()); for(size_t i =0;i\u0026lt;outLayers.size();i++){ names[i] = layersNames[outLayers[i]-1]; } } return names; }  ","date":1547567038,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1547567038,"objectID":"9a57ae8418dd9b6075cc9af6729cd3e4","permalink":"https://RyanAdex.github.io/2019/01/15/opencv-yolov3/","publishdate":"2019-01-15T23:43:58+08:00","relpermalink":"/2019/01/15/opencv-yolov3/","section":"post","summary":"1. 前言 YOLO3能够快速识别图片和视频中的80种物体，而且实时性强，准确度接近SSD。 Opencv是目前最流行的开源图像处理库，使用Open","tags":["opencv","yolov3","C++"],"title":"Opencv+YOLO3目标检测/C++","type":"post"},{"authors":null,"categories":[],"content":" Arch docker的安装 //pacman 安装docker sudo pacman -S docker //docker启动 sudo systemctl start docker //设置开机启动docker sudo systemctl enable docker //将用户添加到docker组 sudo gpasswd -a [usr] docker  //docker 常用命令 docker info docker images docker ps docker -h  docker进行配置 //解决docker pull timeout sudo vi /etc/docker/daemon.json //添加 { \u0026quot;registry-mirrors\u0026quot;: [\u0026quot;https://docker.mirrors.ustc.edu.cn\u0026quot;] } //拉取ubuntu到本地 sudo docker pull ubuntu //查看本地的镜像 sudo docker images REPOSITORY TAG IMAGE ID CREATED SIZE hello-world latest fce289e99eb9 2 weeks ago 1.84kB ubuntu latest 1d9c17228a9e 2 weeks ago 86.7MB //运行镜像，创建一个容器 ： //-p 将本地端口的9900映射到容器端口的8888 // -v 将本地目录/root/JupyterNotebook挂载到容器的/JupyterNotebook 也可以不需要 sudo docker run -it -p 9900:8888 -v /root/JupyterNotebook:/JupyterNotebook 1d9c17228a9e /bin/bash //这时就已经进入容器了 root@1912314cf2b6:/# //更新源 root@1912314cf2b6:/# apt-get update //安装wget下载anaconda root@1912314cf2b6:/# wget https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.sh //运行脚本，一路回车按照提示就行 root@1912314cf2b6:/# bash Anaconda3-2018.12-Linux-x86_64.sh //source一下bash root@1912314cf2b6:/# source /root/.bashrc root@1912314cf2b6:/# rm -rf Anaconda3-2018.12-Linux-x86_64.sh  jupyter Notebook配置 //生成配置文件 root@1912314cf2b6:/# jupyter notebook --generate-config //生成密钥 root@1912314cf2b6:/# ipython Python 3.7.0 (default, Jun 28 2018, 13:15:42) Type 'copyright', 'credits' or 'license' for more information IPython 6.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: from notebook.auth import passwd In [2]: passwd() Enter password: Verify password: Out[2]: 'sha1:4e379475fe85:e9aee4f0b42202fee4f14be37ee8b29ae7dad126' In [3]: exit() //'sha1:4e379475fe85:e9aee4f0b42202fee4f14be37ee8b29ae7dad126'这个东西会写到配置文件里复制下来 //编辑配置文件 root@1912314cf2b6:/# vi /root/.jupyter/jupyter_notebook_config.py //在文件末尾写入（如果后面有需要，看文档就好了） c.NotebookApp.ip='*' # 设置所有ip可以访问 c.NotebookApp.password = u'sha:ce... # 刚才复制的那个密文' c.NotebookApp.open_browser = False # 禁止自动打开浏览器 c.NotebookApp.port =8888 #指定打开的端口 //这里写一个脚本用来启动jupyter notebook //进入容器 docker exec -it ubuntu /bin/bash root@1912314cf2b6:/# vim notebook.sh #!/bin/bash #!/root/.bashrc /root/anaconda3/bin/jupyter notebook --allow-root docker exec -d blissful_blackwell /notebook.sh //使用Jupyter Notebook root@1912314cf2b6:/# jupyter notebook --allow-root [I 08:47:49.861 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret[W 08:47:50.071 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended. [I 08:47:50.113 NotebookApp] JupyterLab extension loaded from /root/anaconda3/lib/python3.7/site-packages/jupyterlab[I 08:47:50.113 NotebookApp] JupyterLab application directory is /root/anaconda3/share/jupyter/lab[I 08:47:50.118 NotebookApp] Serving notebooks from local directory: /[I 08:47:50.118 NotebookApp] The Jupyter Notebook is running at: [I 08:47:50.118 NotebookApp] http://(eb0788fc375d or 127.0.0.1):8888/[I 08:47:50.118 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). //如果访问不了的话，就是主机的端口还没有打开(可以使用telnet来测试你的端口是否允许访问) iptables -I INPUT -p tcp --dport 9900 -m state --state NEW -j ACCEPT iptables-save \u0026gt; /etc/sysconfig/iptables reboot docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESroot@eb0788fc375d:/# eb0788fc375d cd6d8154f1e1 \u0026quot;/bin/bash\u0026quot; 46 minutes ago Up 42 seconds 0.0.0.0:9900-\u0026gt;8888/tcp blissful_blackwell  结束 最后用docker commit 提交更新image\n","date":1547566634,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1547566634,"objectID":"389cc2fef73bcad41ca971a457e337cc","permalink":"https://RyanAdex.github.io/2019/01/15/docker-jupyter/","publishdate":"2019-01-15T23:37:14+08:00","relpermalink":"/2019/01/15/docker-jupyter/","section":"post","summary":"Arch docker的安装 //pacman 安装docker sudo pacman -S docker //docker启动 sudo systemctl start docker //设置开机启动docker sudo systemctl enable docker //将用户添加到docker组","tags":["docker","anaconda","jupyter"],"title":"Docker与anaconda+jupyter","type":"post"},{"authors":null,"categories":[],"content":" 车道检测(Advanced Lane Finding Project) 实现步骤:\n 使用提供的一组棋盘格图片计算相机校正矩阵(camera calibration matrix)和失真系数(distortion coefficients). 校正图片 使用梯度阈值(gradient threshold)，颜色阈值(color threshold)等处理图片得到清晰捕捉车道线的二进制图(binary image). 使用透视变换(perspective transform)得到二进制图(binary image)的鸟瞰图(birds-eye view). 检测属于车道线的像素并用它来测出车道边界. 计算车道曲率及车辆相对车道中央的位置. 处理图片展示车道区域，及车道的曲率和车辆位置.  #forkongithub a{background:#000;color:#fff;text-decoration:none;font-family:arial,sans-serif;text-align:center;font-weight:bold;padding:5px 40px;font-size:0.6rem;line-height:1rem;position:relative;transition:0.5s;}#forkongithub a:hover{background:#c11;color:#fff;}#forkongithub a::before,#forkongithub a::after{content:\u0026ldquo;\u0026rdquo;;width:100%;display:block;position:absolute;top:10px;left:0;height:1px;background:#fff;}#forkongithub a::after{bottom:1px;top:auto;}@media screen and (min-width:800px){#forkongithub{position:fixed;display:block;top:0;left:0;width:200px;overflow:hidden;height:200px;z-index:9999;}#forkongithub a{width:200px;position:absolute;top:80px;left:0px;transform:rotate(0deg);-webkit-transform:rotate(0deg);-ms-transform:rotate(0deg);-moz-transform:rotate(0deg);-o-transform:rotate(0deg);box-shadow:4px 4px 10px rgba(0,0,0,0.8);}}Fork me on GitHub\n相机校正(Camera Calibration) 这里会使用opencv提供的方法通过棋盘格图片组计算相机校正矩阵(camera calibration matrix)和失真系数(distortion coefficients)。首先要得到棋盘格内角的世界坐标\u0026rdquo;object points\u0026rdquo;和对应图片坐标\u0026rdquo;image point\u0026rdquo;。假设棋盘格内角世界坐标的z轴为0，棋盘在(x,y)面上，则对于每张棋盘格图片组的图片而言，对应\u0026rdquo;object points\u0026rdquo;都是一样的。而通过使用openCv的cv::findChessboardCorners()，传入棋盘格的灰度(grayscale)图片和横纵内角点个数就可得到图片内角的\u0026rdquo;image point\u0026rdquo;。\n void get_obj_img_points(const vector\u0026lt;string\u0026gt; \u0026amp; images,const cv::Size \u0026amp; grid,const cv::Size\u0026amp; distance,cv::Mat\u0026amp; cameraMatirx,cv::Mat\u0026amp; distCoeffs){ cv::Mat img,gray;//灰度图像 vector\u0026lt;cv::Point2f\u0026gt; corners;//用来储存t图片角点 vector\u0026lt;cv::Point3f\u0026gt; object_point;//保存标定板上所有角点坐标 vector\u0026lt;cv::Mat\u0026gt; rvecs,tvecs;//旋转向量和位移向量 vector\u0026lt;vector\u0026lt;cv::Point3f\u0026gt;\u0026gt; object_points;//棋盘格三维坐标容器 vector\u0026lt;vector\u0026lt;cv::Point2f\u0026gt;\u0026gt; img_points;//棋盘格角点容器 for(auto \u0026amp; imgdir:images){ //载入图像 img=cv::imread(imgdir); //生成object points for(int i=0;i\u0026lt;grid.height;i++){ for(int j=0;j\u0026lt;grid.width;j++){ object_point.push_back(cv::Point3f(i*distance.width,j*distance.height,0));//向容器存入每个角点坐标 } } //得到灰度图片 cv::cvtColor(img,gray,cv::COLOR_BGR2GRAY); //得到图片的image points //NOTE corners的储存方式为从左往右，从上往下每行储存，所以储存object_point的时候需从grid。width开始遍历储存 bool ret=cv::findChessboardCorners(gray,grid,corners,cv::CALIB_CB_ADAPTIVE_THRESH+cv::CALIB_CB_NORMALIZE_IMAGE+cv::CALIB_CB_FAST_CHECK); if(ret){//亚像素精细化 cv::cornerSubPix(gray,corners,cv::Size(11,11),cv::Size(-1,-1), cv::TermCriteria(cv::TermCriteria::COUNT+cv::TermCriteria::EPS, 30, 0.1)); img_points.push_back(corners); object_points.push_back(object_point); } object_point.clear();//清空object_point以便下一幅图使用该容器 //绘制角点并显示 cv::drawChessboardCorners(img,grid,cv::Mat(corners),ret); // cv::imshow(\u0026quot;chessboard corners\u0026quot;,img); // cv::waitKey(10); } cv::calibrateCamera(object_points,img_points,img.size(),cameraMatirx,distCoeffs,rvecs,tvecs); }  然后使用上方法得到的object_points and img_points 传入cv::calibrateCamera() 方法中就可以计算出相机校正矩阵(camera calibration matrix)和失真系数(distortion coefficients)，再使用 cv::undistort()方法就可得到校正图片。\ndef cal_undistort(img, objpoints, imgpoints): ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img.shape[1::-1], None, None) dst = cv2.undistort(img, mtx, dist, None, mtx) return dst  以下为其中一张棋盘格图片校正前后对比：\n校正测试图片 代码如下：\n//获取棋盘格图片 get_images_by_dir(cal_dir,filetype,imgs); //计算矫正系数 get_obj_img_points(imgs,grid,distance,cameraMatirx,distCoeffs);  测试图片校正前后对比： 阈值过滤(thresholding) 这里会使用梯度阈值(gradient threshold)，颜色阈值(color threshold)等来处理校正后的图片，捕获车道线所在位置的像素。(这里的梯度指的是颜色变化的梯度)\n以下方法通过\u0026rdquo;cv::Sobel()\u0026ldquo;方法计算x轴方向或y轴方向的颜色变化梯度导数，并以此进行阈值过滤(thresholding),得到二进制图(binary image)：\nvoid abs_sobel_thresh(const cv::Mat\u0026amp; src,cv::Mat\u0026amp; dst,const char\u0026amp; orient='x',const int\u0026amp; thresh_min=0,const int\u0026amp; thresh_max=255){ cv::Mat src_gray,grad; cv::Mat abs_gray; //转换成为灰度图片 cv::cvtColor(src,src_gray,cv::COLOR_RGB2GRAY); //使用cv::Sobel()计算x方向或y方向的导 if(orient=='x'){ cv::Sobel(src_gray,grad,CV_64F,1,0); cv::convertScaleAbs(grad,abs_gray); } if(orient=='y'){ cv::Sobel(src_gray,grad,CV_64F,0,1); cv::convertScaleAbs(grad,abs_gray); } //二值化 cv::inRange(abs_gray,thresh_min,thresh_max,dst); // cv::threshold(abs_gray,dst,thresh_min,thresh_max,cv::THRESH_BINARY|cv::THRESH_OTSU); }  通过测试发现使用x轴方向阈值在35到100区间过滤得出的二进制图可以捕捉较为清晰的车道线：\nabs_sobel_thresh(imge,absm,'x',55,200);//sobel边缘识别  以下为使用上面方法应用测试图片的过滤前后对比图： 可以看到该方法的缺陷是在路面颜色相对较浅且车道线颜色为黄色时，无法捕捉到车道线（第三，第六，第七张图），但在其他情况车道线捕捉效果还是不错的。\n接下来测试一下使用全局的颜色变化梯度来进行阈值过滤：\nvoid mag_thresh(const cv::Mat\u0026amp; src,cv::Mat\u0026amp; dst,const int\u0026amp; sobel_kernel=3,const int\u0026amp; thresh_min=0,const int\u0026amp; thresh_max=255){ cv::Mat src_gray,gray_x,gray_y,grad; cv::Mat abs_gray_x,abs_gray_y; //转换成为灰度图片 cv::cvtColor(src,src_gray,cv::COLOR_RGB2GRAY); //使用cv::Sobel()计算x方向或y方向的导 cv::Sobel(src_gray,gray_x,CV_64F,1,0,sobel_kernel); cv::Sobel(src_gray,gray_y,CV_64F,0,1,sobel_kernel); //转换成CV_8U cv::convertScaleAbs(gray_x,abs_gray_x); cv::convertScaleAbs(gray_y,abs_gray_y); //合并x和y方向的梯度 cv::addWeighted(abs_gray_x,0.5,abs_gray_y,0.5,0,grad); //二值化 cv::inRange(grad,thresh_min,thresh_max,dst); // cv::threshold(grad,dst,thresh_min,thresh_max,cv::THRESH_BINARY|cv::THRESH_OTSU); }  mag_thresh(imge,mag,3,45,150);  结果仍然不理想(观察第三，第六，第七张图片)，原因是当路面颜色相对较浅且车道线颜色为黄色时，颜色变化梯度较小，想要把捕捉车道线需要把阈值下限调低，然而这样做同时还会捕获大量的噪音像素，效果会更差。\n那么使用颜色阈值过滤呢？ 下面为使用hls颜色空间的s通道进行阈值过滤：\nvoid hls_select(const cv::Mat\u0026amp; src,cv::Mat\u0026amp; dst,const char\u0026amp; channel='s',const int\u0026amp; thresh_min=0,const int\u0026amp; thresh_max=255){ cv::Mat hls,grad; vector\u0026lt;cv::Mat\u0026gt; channels; cv::cvtColor(src,hls,cv::COLOR_RGB2HLS); //分离通道 cv::split(hls,channels); //选择通道 switch (channel) { case 'h': grad=channels.at(0); break; case 'l': grad=channels.at(1); break; case 's': grad=channels.at(2); break; default: break; } //二值化 cv::inRange(grad,thresh_min,thresh_max,dst); // cv::threshold(grad,dst,thresh_min,thresh_max,cv::THRESH_BINARY); }  mag_thresh(imge,mag,3,45,150);  可以看到在路面颜色相对较浅且车道线颜色为黄色的区域，车道线仍然被清晰的捕捉到了，然而在其他地方表现却不太理想(第四，第八张图片)\n因此为了应对多变的路面情况，需要结合多种阈值过滤方法。\n以下为最终的阈值过滤组合：\nabs_sobel_thresh(imge,absm,'x',55,200);//sobel边缘识别 mag_thresh(imge,mag,3,45,150); hls_select(imge,hls,'s',160,255); dir_threshold(imge,dir,3,0.7,1.3); luv_select(imge,luv,'l',180,255); // lab_select(imge,lab,'b',126,127); imgout=(absm\u0026amp;mag\u0026amp;luv)|(hls\u0026amp;luv);  透视变换(perspective transform) 这里使用\u0026rdquo;cv::getPerspectiveTransform()\u0026ldquo;来获取变形矩阵(tranform matrix)，把阈值过滤后的二进制图片变形为鸟撒视角。\n以下为定义的源点（source points）和目标点（destination points）\n| Source | Destination | |:\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-:|:\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-:| | 585, 460 | 320, 0 | | 203, 720 | 320, 720 | | 1127, 720 | 960, 720 | | 695, 460 | 960, 0 |\n定义方法获取变形矩阵和逆变形矩阵：\nvoid get_M_Minv(const vector\u0026lt;cv::Point2f\u0026gt;\u0026amp; src,const vector\u0026lt;cv::Point2f\u0026gt;\u0026amp; dst,cv::Mat\u0026amp; M,cv::Mat\u0026amp; Minv){ M=cv::getPerspectiveTransform(src,dst); Minv=cv::getPerspectiveTransform(dst,src); }  然后使用\u0026rdquo;cv::warpPerspective()\u0026ldquo;传入相关值获得变形图片(wrapped image)\ncv::warpPerspective(cimg,imge,M,img.size(),cv::INTER_LINEAR);  以下为原图及变形后的效果： 以下为阈值过滤后二进制图变形后效果： 检测车道边界 上面的二进制图还存在一定的噪音像素，为了准确检测车道边界，首先要确定哪些像素是属于车道线的。\n首先要定位车道线的基点(图片最下方车道出现的x轴坐标)，由于车道线在的像素都集中在x轴一定范围内，因此把图片一分为二，左右两边的在x轴上的像素分布峰值非常有可能就是车道线基点。\n以下为测试片x轴的像素分布图：\n定位基点后，再使用使用滑动窗多项式拟合(sliding window polynomial fitting)来获取车道边界。这里使用9个200px宽的滑动窗来定位一条车道线像素：\nvoid find_line(const cv::Mat\u0026amp; src,vector\u0026lt;cv::Point\u0026gt;\u0026amp; lp,vector\u0026lt;cv::Point\u0026gt;\u0026amp; rp,int\u0026amp; rightx_current,int\u0026amp; leftx_current,double\u0026amp; distance_from_center){ cv::Mat hist,nonzero,l,r; vector\u0026lt;cv::Point\u0026gt; nonzerol,nonzeror,lpoint,rpoint; int midpoint; cv::Point leftx_base,rightx_base; //选择滑窗个数 int nwindows = 9; //设置窗口高度 int window_height = int(src.rows/nwindows); //设置窗口宽度 int margin=50; //设置非零像素坐标最少个数 int minpix=50; //TODO 加入if设置图像连续性，如果leftx_current和rightx_current为零，则认为第一次执行，需要计算该两点，如果已经计算了，则不许再次计算。 //rowrange图像区域分割 //将图像处理为一行，以行相加为方法 cv::reduce(src.rowRange(src.rows/2,src.rows),hist,0,cv::REDUCE_SUM,CV_32S); midpoint=int(hist.cols/2); //将hist分为左右分别储存，并找出最大值 //minMaxIdx针对多通道，minMaxLoc针对单通道 cv::minMaxLoc(hist.colRange(0,midpoint),NULL,NULL,NULL,\u0026amp;leftx_base); cv::minMaxLoc(hist.colRange(midpoint,hist.cols),NULL,NULL,NULL,\u0026amp;rightx_base); //左右车道线基础点 leftx_current=leftx_base.x; rightx_current=rightx_base.x+midpoint; // 提前存入该基础点坐标 lpoint.push_back(cv::Point(leftx_current,src.rows)); rpoint.push_back(cv::Point(rightx_current,src.rows)); for(int i=0;i\u0026lt;nwindows;i++){ int win_y_low=src.rows-(i+1)*window_height; //计算选框x坐标点，并将计算结果限制在图像坐标内 int win_xleft_low = leftx_current - margin; win_xleft_low=win_xleft_low\u0026gt;0?win_xleft_low:0; win_xleft_low=win_xleft_low\u0026lt;src.rows?win_xleft_low:src.rows; //int win_xleft_high = leftx_current + margin; int win_xright_low = rightx_current - margin; win_xright_low=win_xright_low\u0026gt;0?win_xright_low:0; win_xright_low=win_xright_low\u0026lt;src.rows?win_xright_low:src.rows; //int win_xright_high = rightx_current + margin; //NOTE要确保参数都大于0，且在src图像范围内，不然会报错 //NOTE 设置为ROI矩形区域选择 l=src(cv::Rect(win_xleft_low,win_y_low,2*margin,window_height)); r=src(cv::Rect(win_xright_low,win_y_low,2*margin,window_height)); //NOTE 把像素值不为零的像素坐标存入矩阵 cv::findNonZero(l,nonzerol); cv::findNonZero(r,nonzeror); //计算每个选框的leftx_current和rightx_current中心点 if(nonzerol.size()\u0026gt;minpix){ int leftx=0; for(auto\u0026amp; n:nonzerol){ leftx+=n.x; } leftx_current=win_xleft_low+leftx/nonzerol.size(); } if(nonzeror.size()\u0026gt;minpix){ int rightx=0; for(auto\u0026amp; n:nonzeror){ rightx+=n.x; } rightx_current=win_xright_low+rightx/nonzeror.size(); } //将中心点坐标存入容器 lpoint.push_back(cv::Point(leftx_current,win_y_low)); rpoint.push_back(cv::Point(rightx_current,win_y_low)); } //拟合左右车道线坐标 cv::Mat leftx = polyfit(lpoint,2); cv::Mat rightx = polyfit(rpoint,2); //计算拟合曲线坐标 lp=polyval(leftx,lpoint,2); rp=polyval(rightx,rpoint,2); //计算车道偏离距离 int lane_width=abs(rpoint.front().x-lpoint.front().x); double lane_xm_per_pix=3.7/lane_width; double veh_pos=(((rpoint.front().x+lpoint.front().x)*lane_xm_per_pix)/2); double cen_pos=((src.cols*lane_xm_per_pix)/2); distance_from_center=veh_pos-cen_pos; // cout\u0026lt;\u0026lt;\u0026quot;dis\u0026quot;\u0026lt;\u0026lt;distance_from_center\u0026lt;\u0026lt;endl; // cout\u0026lt;\u0026lt;lp\u0026lt;\u0026lt;endl; }  以下为滑动窗多项式拟合(sliding window polynomial fitting)得到的结果：\n计算车道曲率及车辆相对车道中心位置 利用检测车道得到的拟合值(find_line 返回的left_fit, right_fit)计算车道曲率，及车辆相对车道中心位置,代码在find_line中：\n int lane_width=abs(rpoint.front().x-lpoint.front().x); double lane_xm_per_pix=3.7/lane_width; double veh_pos=(((rpoint.front().x+lpoint.front().x)*lane_xm_per_pix)/2); double cen_pos=((src.cols*lane_xm_per_pix)/2); distance_from_center=veh_pos-cen_pos;  处理原图，展示信息 使用逆变形矩阵把鸟瞰二进制图检测的车道镶嵌回原图，并高亮车道区域,使用\u0026rdquo;cv::putText()\u0026ldquo;方法处理原图展示车道曲率及车辆相对车道中心位置信息:\nvoid draw_area(const cv::Mat\u0026amp; src,vector\u0026lt;cv::Point\u0026gt;\u0026amp; lp,vector\u0026lt;cv::Point\u0026gt;\u0026amp; rp,const cv::Mat\u0026amp; Minv,double\u0026amp; distance_from_center){ vector\u0026lt;cv::Point\u0026gt; rflip,ptr; cv::Mat colormask=cv::Mat::zeros(src.rows,src.cols,CV_8UC3); cv::Mat dst,midst; //绘制车道线 cv::polylines(colormask,lp,false,cv::Scalar(0,255,0),5); cv::polylines(colormask,rp,false,cv::Scalar(0,0,255),5); //反转坐标，以便绘制填充区域 cv::flip(rp,rflip,1); //拼接坐标 cv::hconcat(lp,rflip,ptr); //绘制填充区域 const cv::Point* em[1]={\u0026amp;ptr[0]}; int nop=(int)ptr.size(); cv::fillPoly(colormask,em,\u0026amp;nop,1,cv::Scalar(200,200,0)); //反变形 cv::warpPerspective(colormask,midst,Minv,src.size(),cv::INTER_LINEAR); //将车道线图片和原始图片叠加 cv::addWeighted(src,1,midst,0.3,0,dst); //绘制文字 cv::putText(dst,\u0026quot;distance bias:\u0026quot;+to_string(distance_from_center)+\u0026quot;m\u0026quot;,cv::Point(50,50),cv::FONT_HERSHEY_SIMPLEX,1,cv::Scalar(255,255,255),2); cv::imshow(\u0026quot;video\u0026quot;,dst); // cv::waitKey(10000); }  以下为测试图片处理后结果：\n以下为处理后测试视频链接:\n处理后视频\n","date":1546762696,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1546762696,"objectID":"3a631c87eeb29e469b4788da5f71fb71","permalink":"https://RyanAdex.github.io/2019/01/06/lanedetection/","publishdate":"2019-01-06T16:18:16+08:00","relpermalink":"/2019/01/06/lanedetection/","section":"post","summary":"车道检测(Advanced Lane Finding Project) 实现步骤: 使用提供的一组棋盘格图片计算相机校正矩阵(camera calibration matrix)和失真系数(distorti","tags":["C++","Lane_detection","opencv"],"title":"车道线检测/Opencv/传统方法","type":"post"},{"authors":null,"categories":[],"content":" 岛屿的个数 给定一个由 \u0026lsquo;1\u0026rsquo;（陆地）和 \u0026lsquo;0\u0026rsquo;（水）组成的的二维网格，计算岛屿的数量。一个岛被水包围，并且它是通过水平方向或垂直方向上相邻的陆地连接而成的。你可以假设网格的四个边均被水包围。 示例1：\n输入: 11110 11010 11000 00000 输出: 1  示例2：\n输入: 11000 11000 00100 00011 输出: 3  分析 这道题的解法有很多，但本帖用广度优先搜索BFS来解答。 本题输入是一个二维数组，判断一个岛屿的要素是判断是否该陆地（1）上下左右是否被水（0）包围，也就是说，岛屿的数量=联通陆地（1）的数量。 BFS算法题解如下，通过找到为岛（1）的初始点，然后对临近的岛屿进行依次访问，利用队列对访问的岛屿进行储存，如下列图示:\n+-----\u0026gt; +-+ ++1|1 1 1 0 +--+ | 1 1 0 1 0 | v 1 1 0 0 0 0 0 0 0 0  当找到初始（1）的时候，将其坐标入队，依据队列的FIFO特性，从队列中取出坐标，对其坐标的上下左右元素进行访问，如果临近的元素为陆地（1），则将其坐标加入队列中等待访问，如果该元素已经被访问，则跳过，重复这一过程，直到队列为空，说明元素周围再也没有陆地，便可看作岛屿。访问过的（1）认为的变为（0）便于后续对未访问的陆地进行查找,岛屿的数量就等于队列为空的遍历次数。其代码如下：\nC++实现 class Solution { private: queue\u0026lt;int\u0026gt; que; int count=0; int x=0; int y=0; int xx=0; int yy=0; public: int numIslands(vector\u0026lt;vector\u0026lt;char\u0026gt;\u0026gt;\u0026amp; grid) { int rows=grid.size(); int cols=rows\u0026gt;0?grid[0].size():0; int dx[]={-1,0,1,0}; int dy[]={0,1,0,-1}; if(rows==0||cols==0){ return 0; } for(int i=0;i\u0026lt;rows;i++){ for(int j=0;j\u0026lt;cols;j++){ //cout\u0026lt;\u0026lt;rows\u0026lt;\u0026lt;cols\u0026lt;\u0026lt;endl;//外部两个for循环为从上到下从左到右寻找未访问的陆地，因为访问过的陆地都已经被置零 if(grid[i][j]=='1'){ que.push(i); que.push(j); grid[i][j]='0'; while(!que.empty()){ x=que.front(); que.pop(); y=que.front(); que.pop(); for(int k=0;k\u0026lt;4;k++){ xx=x+dx[k]; yy=y+dy[k]; if(xx\u0026lt;0||xx\u0026gt;=rows||yy\u0026lt;0||yy\u0026gt;=cols){ continue; } if(grid[xx][yy]=='1'){ grid[xx][yy]='0'; que.push(xx); que.push(yy); } } } count++;//队列为空的次数=岛屿的数量 } } } return count; } };  Go实现 由于go语言没有队列queue包，我们自己建一个：\npackage queue //Item any type's item type Item interface { } //ItemQueue is store items type ItemQueue struct { items []Item } //ItemQueuer is a interface type ItemQueuer interface { New() ItemQueue Push(t Item) Pop() *Item Empty() bool Size() int } //Push a new item func (s *ItemQueue) Push(t Item) { s.items = append(s.items, t) } //Pop a front item func (s *ItemQueue) Pop() { s.items = s.items[1:] } //Empty of items func (s *ItemQueue) Empty() bool { return len(s.items) == 0 } //Size of items func (s *ItemQueue) Size() int { return len(s.items) } //Front of items func (s *ItemQueue) Front() Item { return s.items[0] } //Back of items func (s *ItemQueue) Back() Item { return s.items[len(s.items)-1] }  我们用接口实现了类似C++泛型的queue类，下面是go语言实现：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;self/queue\u0026quot; \u0026quot;time\u0026quot; ) var que queue.ItemQueue//生命一个队列变量 var m = [][]byte{ {'1', '1', '0', '1', '0'}, {'1', '1', '0', '1', '0'}, {'1', '1', '0', '1', '1'}, {'0', '0', '1', '1', '0'}, } func main() { start := time.Now() coun := numIslands(m) fmt.Printf(\u0026quot;the num of isl is %v\u0026quot;, coun) cost := time.Since(start) fmt.Printf(\u0026quot;Cost %s\u0026quot;, cost) } func numIslands(grid [][]byte) int { var que queue.ItemQueue var x, y, xx, yy, count, rows, cols int = 0, 0, 0, 0, 0, 0, 0 rows = len(grid) if rows \u0026gt; 0 { cols = len(grid[0]) } else { cols = 0 } var dx, dy = []int{-1, 0, 1, 0}, []int{0, 1, 0, -1} if rows == 0 || cols == 0 { return 0 } for i := 0; i \u0026lt; rows; i++ { for j := 0; j \u0026lt; cols; j++ { if grid[i][j] == '1' { que.Push(i) que.Push(j) grid[i][j] = '0' for !que.Empty() { x = que.Front().(int)//因为储存的是坐标，所以是int，这里要强制转化，因为que.Front()返回的是interface{}类型 que.Pop() y = que.Front().(int) que.Pop() for k := 0; k \u0026lt; 4; k++ { xx = x + dx[k] yy = y + dy[k] if xx \u0026lt; 0 || xx \u0026gt;= rows || yy \u0026lt; 0 || yy \u0026gt;= cols { continue } if grid[xx][yy] == '1' { grid[xx][yy] = '0' que.Push(xx) que.Push(yy) } } } count++ } } } return count }  ","date":1544358553,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1544358553,"objectID":"4b86190f28bb450e33b7b452e7826fa4","permalink":"https://RyanAdex.github.io/2018/12/09/queue/","publishdate":"2018-12-09T20:29:13+08:00","relpermalink":"/2018/12/09/queue/","section":"post","summary":"岛屿的个数 给定一个由 \u0026lsquo;1\u0026rsquo;（陆地）和 \u0026lsquo;0\u0026rsquo;（水）组成的的二维网格，计算岛屿的数量。一个岛被水","tags":["leetcode","队列","BFS","C++","Go"],"title":"LeetCode 队列与BFS--岛屿的数量","type":"post"},{"authors":null,"categories":[],"content":"前些日子参加了一个叫Advent of Code的编程大赛，每天一道题，快活似神仙。这每道题都有自己的拼图数据输入puzzle input，要做题就需要用到该数据，把数据复制过来感觉又太麻烦，于是就兴起写了一个直接从html读取数据的函数。 其数据如下：\n+12 -10 -4 -8 +18 -1 -13 ...  查看标准库文档，发现net/html包可以做这个功能，其函数如下：\nresp, err := http.Get(\u0026quot;http://example.com/\u0026quot;) if err != nil { // handle error } defer resp.Body.Close() body, err := ioutil.ReadAll(resp.Body) // ...  然后调试，但并未有相关数据输出，在浏览器中检查元素发现该请求需要带cookie才能正确返回数据。直接使用http.Get()并不带有cookie，所以改用NewRequest使用指定的方法、网址和可选的主题创建并返回一个新的*Request。其函数如下：\nclient := \u0026amp;http.Client{} req, err := http.NewRequest(\u0026quot;GET\u0026quot;, url, nil) if err != nil { log.Fatal(err) } req.Header.Set(\u0026quot;Cookie\u0026quot;, \u0026quot;name=value\u0026quot;) resp, err := client.Do(req) robots, err := ioutil.ReadAll(resp.Body) resp.Body.Close()//必须要关闭Body  用ioutil.ReadAll()返回的是[]byte类型，入需要使用可以先将[]byte转换成string，Go语言初学者都会的类型转换语法：string(b)，Go为了稳定性对于上述方法需要经过一些数据上的复制，一旦数据量过大，这个成本是难以忍受的。 所以为了让Go服帖，我们得用上unsafe包，unsafe包提供一些可以跳过Go语言类型安全限制的操作。看下面代码：\nfunc BytesString(b []byte) string { return *(*string)(unsafe.Pointer(\u0026amp;b)) }  我们取到[]byte的指针，Go会说*byte不是*string，但是我们有外挂unsafe.Pointer，所以Go就通过了，接着你很自在的把*byte转成了*string，因为reflect.StringHeader和reflect.SliceHeader的结构体只相差末尾一个字段。 类似的用法可以看Go语言黑魔法 接着用regexp包正则表达式FindAllString取出匹配的[]string数据使用，然后再用strconv.Atoi()进行转化就可以了。 Day1的题为求和，其代码如下：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;io/ioutil\u0026quot; \u0026quot;log\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;os\u0026quot; \u0026quot;regexp\u0026quot; \u0026quot;strconv\u0026quot; \u0026quot;unsafe\u0026quot; ) func BytesString(b []byte) string {//[]byte 转string return *(*string)(unsafe.Pointer(\u0026amp;b)) } func getPuzzle(url string) string {//从网页抓取数据 client := \u0026amp;http.Client{} req, err := http.NewRequest(\u0026quot;GET\u0026quot;, url, nil) if err != nil { log.Fatal(err) } req.Header.Set(\u0026quot;Cookie\u0026quot;, \u0026quot;session=53616c74...\u0026quot;) resp, err := client.Do(req) robots, err := ioutil.ReadAll(resp.Body) // robots := bufio.NewReader(resp.Body) resp.Body.Close() str := BytesString(robots) // fmt.Printf(\u0026quot;%q\u0026quot;, str) return str // io.Copy(os.Stdout, resp.Body) } func sum(num []int) int { sum := 0 for _, n := range num { sum += n } return sum } func strtoint(str []string) []int {//string转int num := make([]int, len(str)) for i := 0; i \u0026lt; len(str); i++ { flag := str[i][0] chafre, _ := strconv.Atoi(str[i][1:]) switch flag { case '+': num[i] = chafre case '-': num[i] = -chafre } } return num } func main() { change := getPuzzle(\u0026quot;https://adventofcode.com/2018/day/1/input\u0026quot;) re := regexp.MustCompile(\u0026quot;[+|-][0-9]*\u0026quot;)//正则表达式 str := re.FindAllString(change, -1) num := strtoint(str) sum := sum(num) fmt.Printf(\u0026quot;sum is %d\\n\u0026quot;, sum) }  如果有更好的方法，欢迎私信，哈哈哈···\n","date":1543926587,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1543926587,"objectID":"94d356670112590754f4cc52c9561bc9","permalink":"https://RyanAdex.github.io/2018/12/04/htmlparse/","publishdate":"2018-12-04T20:29:47+08:00","relpermalink":"/2018/12/04/htmlparse/","section":"post","summary":"前些日子参加了一个叫Advent of Code的编程大赛，每天一道题，快活似神仙。这每道题都有自己的拼图数据输入puzzle input，要做题就","tags":["Go","html","cookie","BytesString"],"title":"go语言带cookie的net客户端请求ii与[]byte转string","type":"post"},{"authors":null,"categories":[],"content":"时值我小病在家休养生息，喜欢跳广场舞的外公来寻求我的帮助，他们跳广场舞是将存有歌曲的U盘插到音响上面，而音响大部分都是只能显示歌曲的索引index，不能直接显示歌曲名，所以为了方便他们会在U盘里面对歌曲进行排序。由于音响是寻址按顺序播放，意思就是在U盘里面的歌曲需要一首一首的按顺序复制过去，而且当对U盘歌曲进行增添的时候又需要按照顺序重新复制一遍，可以说相当麻烦。为了将我从这重复的劳动中解放出来，我用go语言写了一个小工具，本来想着分分钟写完，却没想到踩到了坑。 在os包中有一个Rename()函数具有重命名和移动的功能，其函数如下：\nfunc Rename(oldpath, newpath string) error  Rename修改一个文件的名字，移动一个文件。可能会有一些个操作系统特定的限制。\n在windows系统下面使用该函数，oldpath和newpath在同一个磁盘/卷下面能正常使用，可我需要将音乐移动到U盘上，当使用这个函数的时候出现了：\nThe system cannot move the file to a different disk drive.  搜索Github的issues，发现Rename在windows中不能进行跨磁盘/卷操作。 为了实现跨磁盘/卷操作，一种方法是直接调用windows API，于是在windows api docs中搜索到movefileex()函数能够实现该功能，但是在go语言的syscall包中只有movefile()函数，其函数如下：\nfunc MoveFile(from *uint16, to *uint16) (err error)  实现文件移动的函数可以写成：\nfunc movefile(oldpath, newpath string) error { //跨卷移动 from, err := syscall.UTF16PtrFromString(oldpath) if err != nil { return err } to, err := syscall.UTF16PtrFromString(newpath) if err != nil { return err } return syscall.MoveFile(from, to)//windows API }  当我们读取oldpath目录里面的文件时调用io/ioutil包的ReadDir()函数可按照文件夹内的排序方式批量读入文件名，其函数实现如下：\nfunc filenamelist(filepath string) []string { var list []string rd, err := ioutil.ReadDir(filepath) //遍历目录 if err != nil { log.Fatal(err) } for _, fi := range rd { if fi.IsDir() { fmt.Printf(\u0026quot;[%s]is dir\\n\u0026quot;, fi.Name()) } else { list = append(list, fi.Name()) fmt.Println(filepath + fi.Name()) } } return list//返回一个string数组 }  当然也可以普通方式读入文件名然后用sort包进行排序。 下面是完整代码：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;io/ioutil\u0026quot; \u0026quot;log\u0026quot; \u0026quot;os\u0026quot; \u0026quot;syscall\u0026quot; ) func movefile(oldpath, newpath string) error { //跨卷移动 from, err := syscall.UTF16PtrFromString(oldpath) if err != nil { return err } to, err := syscall.UTF16PtrFromString(newpath) if err != nil { return err } return syscall.MoveFile(from, to)//windows API } func filenamelist(filepath string) []string { var list []string rd, err := ioutil.ReadDir(filepath) //遍历目录 if err != nil { log.Fatal(err) } for _, fi := range rd { if fi.IsDir() { fmt.Printf(\u0026quot;[%s]is dir\\n\u0026quot;, fi.Name()) } else { list = append(list, fi.Name()) fmt.Println(filepath + fi.Name()) } } return list } func movefilelist(oldpath, newpath string) { for _, fi := range filenamelist(oldpath) { //移动目录的所有文件 err := movefile(oldpath+fi, newpath+fi) if err != nil { log.Fatal(err) } fmt.Println(fi + \u0026quot;--Move To --\u0026gt;\u0026quot; + newpath + \u0026quot;--OK!\u0026quot;) } } func main() { movefilelist(os.Args[1], os.Args[2]) }  最后在终端中：\nRyan@DESKTO MINGW64 /Go/test (master) $ go run main.go ~/Desktop/music/ /d/ C:/Users/Ryan/Desktop/music/新建文本文档 (2).txt C:/Users/Ryan/Desktop/music/新建文本文档.txt 新建文本文档 (2).txt--Move To --\u0026gt;D:/--OK! 新建文本文档.txt--Move To --\u0026gt;D:/--OK!  有了这个脚本，我今后终于可以愉快的帮助我外公了。\n转载请注明\n","date":1543382926,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1543382926,"objectID":"a137288f40059c00aaaa4b67c9608a21","permalink":"https://RyanAdex.github.io/2018/11/28/filemove/","publishdate":"2018-11-28T13:28:46+08:00","relpermalink":"/2018/11/28/filemove/","section":"post","summary":"时值我小病在家休养生息，喜欢跳广场舞的外公来寻求我的帮助，他们跳广场舞是将存有歌曲的U盘插到音响上面，而音响大部分都是只能显示歌曲的索引in","tags":["Go","windows"],"title":"go语言 os.Rename() cannot move the file to a different disk drive 怎么办","type":"post"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536422400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1536422400,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"https://RyanAdex.github.io/tutorial/example/","publishdate":"2018-09-09T00:00:00+08:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"}]